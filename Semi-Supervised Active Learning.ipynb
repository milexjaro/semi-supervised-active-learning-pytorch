{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6612ba4",
   "metadata": {},
   "source": [
    "This notebook serves as the Deep Bayesian Active Learning paper implementation based on:\n",
    "- https://github.com/damienlancry/DBAL/blob/master/dbal_pytorch.ipynb\n",
    "- https://github.com/wohlert/semi-supervised-pytorch/blob/master/examples/notebooks/Deep%20Generative%20Model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6582b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56fa5b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from skorch import NeuralNetClassifier\n",
    "from modAL.models import ActiveLearner\n",
    "from torchsummary import summary\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8800ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cuda if available\n",
    "cuda = torch.cuda.is_available()\n",
    "# Add a semi supervised module\n",
    "sys.path.insert(0, './semi-supervised')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0425ef",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349924b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(CNN, self).__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "                        nn.Conv2d(1, 32, 4),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(32, 32, 4),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2),\n",
    "                        nn.Dropout(0.25),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(11*11*32, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.5),\n",
    "                        nn.Linear(128, 10)\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.convs(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "539f8cb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 25, 25]             544\n",
      "              ReLU-2           [-1, 32, 25, 25]               0\n",
      "            Conv2d-3           [-1, 32, 22, 22]          16,416\n",
      "              ReLU-4           [-1, 32, 22, 22]               0\n",
      "         MaxPool2d-5           [-1, 32, 11, 11]               0\n",
      "           Dropout-6           [-1, 32, 11, 11]               0\n",
      "           Flatten-7                 [-1, 3872]               0\n",
      "            Linear-8                  [-1, 128]         495,744\n",
      "              ReLU-9                  [-1, 128]               0\n",
      "          Dropout-10                  [-1, 128]               0\n",
      "           Linear-11                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 513,994\n",
      "Trainable params: 513,994\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.63\n",
      "Params size (MB): 1.96\n",
      "Estimated Total Size (MB): 2.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(CNN().convs, (1,28,28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa309851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "train = MNIST('.', train=True, download=True, transform=ToTensor())\n",
    "test = MNIST('.', train=False, download=True, transform=ToTensor())\n",
    "train_data_loader = DataLoader(train, shuffle = True, batch_size = 60_000)\n",
    "test_data_loader = DataLoader(test, shuffle = True, batch_size = 10_000)\n",
    "X_train, y_train = next(iter(train_data_loader))\n",
    "X_train = X_train.detach().cpu().numpy()\n",
    "y_train = y_train.detach().cpu().numpy()\n",
    "\n",
    "X_test, y_test = next(iter(test_data_loader))\n",
    "X_test = X_test.detach().cpu().numpy()\n",
    "y_test = y_test.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537453a",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f454c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60_000, 1, 28, 28)\n",
    "X_test = X_test.reshape(10_000, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ac0a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial labelled data\n",
    "# Take 2 samples from each digit\n",
    "initial_labelled_idx = np.array([],dtype=np.int64)\n",
    "for i in range(10):\n",
    "    idx = np.random.choice(np.where(y_train==i)[0], size=10, replace=False)\n",
    "    initial_labelled_idx = np.concatenate((initial_labelled_idx, idx))\n",
    "\n",
    "X_train_labelled_initial = X_train[initial_labelled_idx]\n",
    "y_train_labelled_initial = y_train[initial_labelled_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dc03dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pool of unlabelled data\n",
    "X_train_unlabelled = np.delete(X_train, initial_labelled_idx, axis = 0)\n",
    "y_train_unlabelled = np.delete(y_train, initial_labelled_idx, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "921f0376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_labelled_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a499c75",
   "metadata": {},
   "source": [
    "### Query Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81f0dc",
   "metadata": {},
   "source": [
    "#### Random (Uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "301a3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(learner, X, n_instances = 1):\n",
    "    query_idx = np.random.choice(range(len(X)), size = n_instances, replace = False)\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2d126",
   "metadata": {},
   "source": [
    "#### Max Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ee8320",
   "metadata": {},
   "source": [
    "Formula:\n",
    "\n",
    "$$\\mathbb{H} = -\\sum_{c}p_clog(p_c)$$\n",
    "\n",
    "Where $p_c$ is the probability of the instance being in the class $c$ and is approximated by:\n",
    "\n",
    "$$p_c = \\dfrac{1}{T}\\sum_{c}p_c^{(t)}$$\n",
    "\n",
    "Where $p_c^{(t)}$ is the probability of the instance being in the class $c$ at t-th forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "debe359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_entropy(learner, X, n_instances=1, T=100):\n",
    "    random_subset = np.random.choice(range(len(X)), size=2000, replace=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = np.stack([torch.softmax(learner.estimator.forward(X[random_subset], training=True),dim=-1).cpu().numpy()\n",
    "                            for t in range(100)])\n",
    "    pc = outputs.mean(axis=0)\n",
    "    acquisition = (-pc*np.log(pc + 1e-10)).sum(axis=-1)\n",
    "    idx = (-acquisition).argsort()[:n_instances]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ead77e",
   "metadata": {},
   "source": [
    "#### Bald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48e2dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bald(learner, X, n_instances=1, T=100):\n",
    "    random_subset = np.random.choice(range(len(X)), size=2000, replace=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = np.stack([torch.softmax(learner.estimator.forward(X[random_subset], training=True),dim=-1).cpu().numpy()\n",
    "                            for t in range(100)])\n",
    "    pc = outputs.mean(axis=0)\n",
    "    H   = (-pc*np.log(pc + 1e-10)).sum(axis=-1)\n",
    "    E_H = - np.mean(np.sum(outputs * np.log(outputs + 1e-10), axis=-1), axis=0)  # [batch size]\n",
    "    acquisition = H - E_H\n",
    "    idx = (-acquisition).argsort()[:n_instances]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X[query_idx]    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6640132",
   "metadata": {},
   "source": [
    "### Active Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b42c881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "query_strategies = {'random':{'function':uniform,\n",
    "                                'classifier': None,\n",
    "                                'learner': None,\n",
    "                                'performance_history': None},\n",
    "                    # 'max_entropy':{'function':max_entropy,\n",
    "                    #             'classifier': None,\n",
    "                    #             'learner': None,\n",
    "                    #             'performance_history': None},\n",
    "                    'bald':{'function':bald,\n",
    "                                'classifier': None,\n",
    "                                'learner': None,\n",
    "                                'performance_history': None},\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8db2c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_procedure(query_strategy,\n",
    "                              X_test,\n",
    "                              y_test,\n",
    "                              X_pool,\n",
    "                              y_pool,\n",
    "                              X_initial,\n",
    "                              y_initial,\n",
    "                              estimator,\n",
    "                              n_queries=100,\n",
    "                              n_instances=10,\n",
    "                              query_strategy_label=''):\n",
    "    learner = ActiveLearner(estimator=estimator,\n",
    "                            X_training=X_initial,\n",
    "                            y_training=y_initial,\n",
    "                            query_strategy=query_strategy,\n",
    "                           )\n",
    "    perf_hist = [learner.score(X_test, y_test)]\n",
    "    for index in range(n_queries):\n",
    "        query_idx, query_instance = learner.query(X_pool, n_instances)\n",
    "        learner.teach(X_pool[query_idx], y_pool[query_idx])\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
    "        model_accuracy = learner.score(X_test, y_test)\n",
    "        print('{q}Accuracy after query {n}: {acc:0.4f}'.format(q=f'[{query_strategy_label}] ', n=index + 1, acc=model_accuracy))\n",
    "        perf_hist.append(model_accuracy)\n",
    "\n",
    "    # Save after-query labelled and unlabelled data\n",
    "    torch.save(learner.X_training, f'./saved_data/X_train_labelled_{query_strategy_label}.pt')\n",
    "    torch.save(learner.y_training, f'./saved_data/y_train_labelled_{query_strategy_label}.pt')\n",
    "\n",
    "    torch.save(X_pool, f'./saved_data/X_train_unlabelled_{query_strategy_label}.pt')\n",
    "    torch.save(y_pool, f'./saved_data/y_train_unlabelled_{query_strategy_label}.pt')\n",
    "\n",
    "    return learner, perf_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dba7250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[random] Accuracy after query 1: 0.8198\n",
      "[random] Accuracy after query 2: 0.8229\n",
      "[random] Accuracy after query 3: 0.8270\n",
      "[random] Accuracy after query 4: 0.8560\n",
      "[random] Accuracy after query 5: 0.8347\n",
      "[random] Accuracy after query 6: 0.8529\n",
      "[random] Accuracy after query 7: 0.8513\n",
      "[random] Accuracy after query 8: 0.8553\n",
      "[random] Accuracy after query 9: 0.8538\n",
      "[random] Accuracy after query 10: 0.8610\n",
      "[random] Accuracy after query 11: 0.8652\n",
      "[random] Accuracy after query 12: 0.8729\n",
      "[random] Accuracy after query 13: 0.8689\n",
      "[random] Accuracy after query 14: 0.8818\n",
      "[random] Accuracy after query 15: 0.8687\n",
      "[random] Accuracy after query 16: 0.8864\n",
      "[random] Accuracy after query 17: 0.8753\n",
      "[random] Accuracy after query 18: 0.8866\n",
      "[random] Accuracy after query 19: 0.8815\n",
      "[random] Accuracy after query 20: 0.8780\n",
      "[random] Accuracy after query 21: 0.8853\n",
      "[random] Accuracy after query 22: 0.8949\n",
      "[random] Accuracy after query 23: 0.8982\n",
      "[random] Accuracy after query 24: 0.8934\n",
      "[random] Accuracy after query 25: 0.9034\n",
      "[random] Accuracy after query 26: 0.9051\n",
      "[random] Accuracy after query 27: 0.8969\n",
      "[random] Accuracy after query 28: 0.9131\n",
      "[random] Accuracy after query 29: 0.9108\n",
      "[random] Accuracy after query 30: 0.9121\n",
      "[random] Accuracy after query 31: 0.9187\n",
      "[random] Accuracy after query 32: 0.9182\n",
      "[random] Accuracy after query 33: 0.9215\n",
      "[random] Accuracy after query 34: 0.9186\n",
      "[random] Accuracy after query 35: 0.9276\n",
      "[random] Accuracy after query 36: 0.9186\n",
      "[random] Accuracy after query 37: 0.9244\n",
      "[random] Accuracy after query 38: 0.9196\n",
      "[random] Accuracy after query 39: 0.9258\n",
      "[random] Accuracy after query 40: 0.9216\n",
      "[random] Accuracy after query 41: 0.9161\n",
      "[random] Accuracy after query 42: 0.9217\n",
      "[random] Accuracy after query 43: 0.9224\n",
      "[random] Accuracy after query 44: 0.9282\n",
      "[random] Accuracy after query 45: 0.9244\n",
      "[random] Accuracy after query 46: 0.9301\n",
      "[random] Accuracy after query 47: 0.9239\n",
      "[random] Accuracy after query 48: 0.9317\n",
      "[random] Accuracy after query 49: 0.9340\n",
      "[random] Accuracy after query 50: 0.9362\n",
      "[random] Accuracy after query 51: 0.9351\n",
      "[random] Accuracy after query 52: 0.9357\n",
      "[random] Accuracy after query 53: 0.9314\n",
      "[random] Accuracy after query 54: 0.9354\n",
      "[random] Accuracy after query 55: 0.9299\n",
      "[random] Accuracy after query 56: 0.9335\n",
      "[random] Accuracy after query 57: 0.9387\n",
      "[random] Accuracy after query 58: 0.9329\n",
      "[random] Accuracy after query 59: 0.9387\n",
      "[random] Accuracy after query 60: 0.9373\n",
      "[random] Accuracy after query 61: 0.9428\n",
      "[random] Accuracy after query 62: 0.9449\n",
      "[random] Accuracy after query 63: 0.9361\n",
      "[random] Accuracy after query 64: 0.9420\n",
      "[random] Accuracy after query 65: 0.9418\n",
      "[random] Accuracy after query 66: 0.9399\n",
      "[random] Accuracy after query 67: 0.9358\n",
      "[random] Accuracy after query 68: 0.9425\n",
      "[random] Accuracy after query 69: 0.9358\n",
      "[random] Accuracy after query 70: 0.9406\n",
      "[random] Accuracy after query 71: 0.9386\n",
      "[random] Accuracy after query 72: 0.9415\n",
      "[random] Accuracy after query 73: 0.9426\n",
      "[random] Accuracy after query 74: 0.9428\n",
      "[random] Accuracy after query 75: 0.9440\n",
      "[random] Accuracy after query 76: 0.9369\n",
      "[random] Accuracy after query 77: 0.9413\n",
      "[random] Accuracy after query 78: 0.9391\n",
      "[random] Accuracy after query 79: 0.9463\n",
      "[random] Accuracy after query 80: 0.9416\n",
      "[random] Accuracy after query 81: 0.9452\n",
      "[random] Accuracy after query 82: 0.9431\n",
      "[random] Accuracy after query 83: 0.9420\n",
      "[random] Accuracy after query 84: 0.9472\n",
      "[random] Accuracy after query 85: 0.9463\n",
      "[random] Accuracy after query 86: 0.9471\n",
      "[random] Accuracy after query 87: 0.9483\n",
      "[random] Accuracy after query 88: 0.9428\n",
      "[random] Accuracy after query 89: 0.9466\n",
      "[random] Accuracy after query 90: 0.9463\n",
      "[random] Accuracy after query 91: 0.9479\n",
      "[random] Accuracy after query 92: 0.9464\n",
      "[random] Accuracy after query 93: 0.9491\n",
      "[random] Accuracy after query 94: 0.9493\n",
      "[random] Accuracy after query 95: 0.9491\n",
      "[random] Accuracy after query 96: 0.9496\n",
      "[random] Accuracy after query 97: 0.9512\n",
      "[random] Accuracy after query 98: 0.9537\n",
      "[random] Accuracy after query 99: 0.9507\n",
      "[random] Accuracy after query 100: 0.9492\n",
      "[bald] Accuracy after query 1: 0.8303\n",
      "[bald] Accuracy after query 2: 0.8275\n",
      "[bald] Accuracy after query 3: 0.8521\n",
      "[bald] Accuracy after query 4: 0.8192\n",
      "[bald] Accuracy after query 5: 0.8565\n",
      "[bald] Accuracy after query 6: 0.8542\n",
      "[bald] Accuracy after query 7: 0.8590\n",
      "[bald] Accuracy after query 8: 0.8606\n",
      "[bald] Accuracy after query 9: 0.8802\n",
      "[bald] Accuracy after query 10: 0.9002\n",
      "[bald] Accuracy after query 11: 0.8956\n",
      "[bald] Accuracy after query 12: 0.8973\n",
      "[bald] Accuracy after query 13: 0.9134\n",
      "[bald] Accuracy after query 14: 0.9189\n",
      "[bald] Accuracy after query 15: 0.9148\n",
      "[bald] Accuracy after query 16: 0.9036\n",
      "[bald] Accuracy after query 17: 0.9237\n",
      "[bald] Accuracy after query 18: 0.9295\n",
      "[bald] Accuracy after query 19: 0.9287\n",
      "[bald] Accuracy after query 20: 0.9241\n",
      "[bald] Accuracy after query 21: 0.9244\n",
      "[bald] Accuracy after query 22: 0.9342\n",
      "[bald] Accuracy after query 23: 0.9376\n",
      "[bald] Accuracy after query 24: 0.9361\n",
      "[bald] Accuracy after query 25: 0.9358\n",
      "[bald] Accuracy after query 26: 0.9480\n",
      "[bald] Accuracy after query 27: 0.9450\n",
      "[bald] Accuracy after query 28: 0.9524\n",
      "[bald] Accuracy after query 29: 0.9484\n",
      "[bald] Accuracy after query 30: 0.9440\n",
      "[bald] Accuracy after query 31: 0.9453\n",
      "[bald] Accuracy after query 32: 0.9505\n",
      "[bald] Accuracy after query 33: 0.9569\n",
      "[bald] Accuracy after query 34: 0.9569\n",
      "[bald] Accuracy after query 35: 0.9595\n",
      "[bald] Accuracy after query 36: 0.9556\n",
      "[bald] Accuracy after query 37: 0.9617\n",
      "[bald] Accuracy after query 38: 0.9471\n",
      "[bald] Accuracy after query 39: 0.9619\n",
      "[bald] Accuracy after query 40: 0.9615\n",
      "[bald] Accuracy after query 41: 0.9581\n",
      "[bald] Accuracy after query 42: 0.9577\n",
      "[bald] Accuracy after query 43: 0.9571\n",
      "[bald] Accuracy after query 44: 0.9559\n",
      "[bald] Accuracy after query 45: 0.9625\n",
      "[bald] Accuracy after query 46: 0.9658\n",
      "[bald] Accuracy after query 47: 0.9629\n",
      "[bald] Accuracy after query 48: 0.9655\n",
      "[bald] Accuracy after query 49: 0.9670\n",
      "[bald] Accuracy after query 50: 0.9650\n",
      "[bald] Accuracy after query 51: 0.9687\n",
      "[bald] Accuracy after query 52: 0.9641\n",
      "[bald] Accuracy after query 53: 0.9665\n",
      "[bald] Accuracy after query 54: 0.9667\n",
      "[bald] Accuracy after query 55: 0.9708\n",
      "[bald] Accuracy after query 56: 0.9693\n",
      "[bald] Accuracy after query 57: 0.9695\n",
      "[bald] Accuracy after query 58: 0.9711\n",
      "[bald] Accuracy after query 59: 0.9740\n",
      "[bald] Accuracy after query 60: 0.9676\n",
      "[bald] Accuracy after query 61: 0.9733\n",
      "[bald] Accuracy after query 62: 0.9733\n",
      "[bald] Accuracy after query 63: 0.9743\n",
      "[bald] Accuracy after query 64: 0.9752\n",
      "[bald] Accuracy after query 65: 0.9708\n",
      "[bald] Accuracy after query 66: 0.9751\n",
      "[bald] Accuracy after query 67: 0.9701\n",
      "[bald] Accuracy after query 68: 0.9717\n",
      "[bald] Accuracy after query 69: 0.9749\n",
      "[bald] Accuracy after query 70: 0.9769\n",
      "[bald] Accuracy after query 71: 0.9766\n",
      "[bald] Accuracy after query 72: 0.9743\n",
      "[bald] Accuracy after query 73: 0.9796\n",
      "[bald] Accuracy after query 74: 0.9764\n",
      "[bald] Accuracy after query 75: 0.9757\n",
      "[bald] Accuracy after query 76: 0.9786\n",
      "[bald] Accuracy after query 77: 0.9785\n",
      "[bald] Accuracy after query 78: 0.9762\n",
      "[bald] Accuracy after query 79: 0.9779\n",
      "[bald] Accuracy after query 80: 0.9787\n",
      "[bald] Accuracy after query 81: 0.9797\n",
      "[bald] Accuracy after query 82: 0.9783\n",
      "[bald] Accuracy after query 83: 0.9813\n",
      "[bald] Accuracy after query 84: 0.9802\n",
      "[bald] Accuracy after query 85: 0.9793\n",
      "[bald] Accuracy after query 86: 0.9793\n",
      "[bald] Accuracy after query 87: 0.9812\n",
      "[bald] Accuracy after query 88: 0.9793\n",
      "[bald] Accuracy after query 89: 0.9825\n",
      "[bald] Accuracy after query 90: 0.9823\n",
      "[bald] Accuracy after query 91: 0.9803\n",
      "[bald] Accuracy after query 92: 0.9781\n",
      "[bald] Accuracy after query 93: 0.9734\n",
      "[bald] Accuracy after query 94: 0.9797\n",
      "[bald] Accuracy after query 95: 0.9786\n",
      "[bald] Accuracy after query 96: 0.9826\n",
      "[bald] Accuracy after query 97: 0.9821\n",
      "[bald] Accuracy after query 98: 0.9828\n",
      "[bald] Accuracy after query 99: 0.9832\n",
      "[bald] Accuracy after query 100: 0.9802\n"
     ]
    }
   ],
   "source": [
    "for query_strategy_name, query_strategy in query_strategies.items():\n",
    "    query_strategy['classifier'] = NeuralNetClassifier(CNN,\n",
    "                                max_epochs=50,\n",
    "                                batch_size=128,\n",
    "                                lr=0.001,\n",
    "                                optimizer=torch.optim.Adam,\n",
    "                                criterion=torch.nn.CrossEntropyLoss,\n",
    "                                train_split=None,\n",
    "                                verbose=0,\n",
    "                                device=device)\n",
    "    query_strategy['learner'], query_strategy['performance_history'] = active_learning_procedure(query_strategy['function'],\n",
    "                                                X_test,\n",
    "                                                y_test,\n",
    "                                                X_train_unlabelled,\n",
    "                                                y_train_unlabelled,\n",
    "                                                X_train_labelled_initial,\n",
    "                                                y_train_labelled_initial,\n",
    "                                                query_strategy['classifier'],\n",
    "                                                query_strategy_label=query_strategy_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69232167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and data\n",
    "performance_histories = {}\n",
    "for query_strategy_name, query_strategy in query_strategies.items():\n",
    "    # Save classifiers\n",
    "    query_strategy['classifier'].save_params(f_params=f'./saved_models/{query_strategy_name}.pkl')\n",
    "\n",
    "    # Save data after queries:\n",
    "    # torch.save(query_strategy['learner'].X_training, f'./saved_data/X_training_{query_strategy_name}.pt')\n",
    "    # torch.save(query_strategy['learner'].y_training, f'./saved_data/y_training_{query_strategy_name}.pt')\n",
    "\n",
    "# Save initial labelled and unlabelled data\n",
    "X_train_labelled_initial = X_train[initial_labelled_idx]\n",
    "y_train_labelled_initial = y_train[initial_labelled_idx]\n",
    "torch.save(X_train_labelled_initial, f'./saved_data/X_train_labelled.pt')\n",
    "torch.save(y_train_labelled_initial, f'./saved_data/y_train_labelled.pt')\n",
    "torch.save(X_train_unlabelled, f'./saved_data/X_train_unlabelled.pt')\n",
    "torch.save(y_train_unlabelled, f'./saved_data/y_train_unlabelled.pt')\n",
    "\n",
    "# Save test data\n",
    "torch.save(X_test, f'./saved_data/X_test.pt')\n",
    "torch.save(y_test, f'./saved_data/y_test.pt')\n",
    "\n",
    "# Save performance histories (preprocessing)\n",
    "performance_histories = {query_strategy_name: query_strategy['performance_history'] for query_strategy_name, query_strategy in query_strategies.items()}\n",
    "pd.DataFrame(performance_histories).to_csv(f'./saved_data/active_learning_performance_histories.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6654fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7232ffad00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "sns.set()\n",
    "# plt.plot(query_strategies['max_entropy']['performance_history'], label=\"max entropy\")\n",
    "plt.plot(query_strategies['bald']['performance_history'], label=\"bald\")\n",
    "plt.plot(query_strategies['random']['performance_history'], label=\"random\")\n",
    "plt.ylim([0.7,1])\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e79de",
   "metadata": {},
   "source": [
    "###  Semi-supervised Learning\n",
    "\n",
    "<img src=\"./images/dgm.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042cd798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ucl/dissertation/./semi-supervised/models/vae.py:114: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  init.xavier_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "from models import DeepGenerativeModel\n",
    "\n",
    "x_dim = 784\n",
    "y_dim = 10\n",
    "z_dim = 32\n",
    "h_dim = [256, 128]\n",
    "\n",
    "model = DeepGenerativeModel([x_dim, y_dim, z_dim, h_dim])\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b66488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import SemiSupervisedActiveLearningDataset\n",
    "from utils import onehot\n",
    "flatten_bernoulli = lambda x: transforms.ToTensor()(x).view(-1).bernoulli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4edae815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./saved_data/X_train_labelled_bald.pt\n",
      "./saved_data/y_train_labelled_bald.pt\n",
      "1100 1100\n",
      "./saved_data/X_train_unlabelled_bald.pt\n",
      "./saved_data/y_train_unlabelled_bald.pt\n",
      "58900 58900\n",
      "./saved_data/X_test.pt\n",
      "./saved_data/y_test.pt\n",
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "from datautils import get_mnist\n",
    "\n",
    "# Only use 10 labelled examples per class\n",
    "# The rest of the data is unlabelled.\n",
    "labelled, unlabelled, test = get_mnist(location=\"./saved_data\", batch_size=64, labels_per_class=10, algorithm='bald')\n",
    "alpha = 0.1 * len(unlabelled) / len(labelled)\n",
    "\n",
    "def binary_cross_entropy(y_pred, y):\n",
    "    return -torch.sum(y * torch.log(y_pred + 1e-8) + (1 - y) * torch.log(1 - y_pred + 1e-8), dim=-1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5563ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from inference import SVI, ImportanceWeightedSampler\n",
    "\n",
    "# You can use importance weighted samples [Burda, 2015] to get a better estimate\n",
    "# on the log-likelihood.\n",
    "sampler = ImportanceWeightedSampler(mc=1, iw=1)\n",
    "\n",
    "if cuda: model = model.cuda()\n",
    "elbo = SVI(model, likelihood=binary_cross_entropy, sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d45d84fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "[Train]\t\t J_a: 400.76, accuracy: 0.82\n",
      "[Validation]\t J_a: 318.45, accuracy: 0.83\n",
      "Epoch: 1\n",
      "[Train]\t\t J_a: 321.91, accuracy: 0.97\n",
      "[Validation]\t J_a: 293.73, accuracy: 0.88\n",
      "Epoch: 2\n",
      "[Train]\t\t J_a: 304.59, accuracy: 0.99\n",
      "[Validation]\t J_a: 282.21, accuracy: 0.90\n",
      "Epoch: 3\n",
      "[Train]\t\t J_a: 293.93, accuracy: 1.00\n",
      "[Validation]\t J_a: 275.59, accuracy: 0.91\n",
      "Epoch: 4\n",
      "[Train]\t\t J_a: 287.56, accuracy: 1.00\n",
      "[Validation]\t J_a: 270.48, accuracy: 0.92\n",
      "Epoch: 5\n",
      "[Train]\t\t J_a: 283.18, accuracy: 1.00\n",
      "[Validation]\t J_a: 266.64, accuracy: 0.93\n",
      "Epoch: 6\n",
      "[Train]\t\t J_a: 279.42, accuracy: 1.00\n",
      "[Validation]\t J_a: 263.19, accuracy: 0.93\n",
      "Epoch: 7\n",
      "[Train]\t\t J_a: 276.60, accuracy: 1.00\n",
      "[Validation]\t J_a: 261.85, accuracy: 0.94\n",
      "Epoch: 8\n",
      "[Train]\t\t J_a: 273.27, accuracy: 1.00\n",
      "[Validation]\t J_a: 259.64, accuracy: 0.94\n",
      "Epoch: 9\n",
      "[Train]\t\t J_a: 271.91, accuracy: 1.00\n",
      "[Validation]\t J_a: 257.73, accuracy: 0.94\n",
      "Epoch: 10\n",
      "[Train]\t\t J_a: 269.85, accuracy: 1.00\n",
      "[Validation]\t J_a: 257.04, accuracy: 0.95\n",
      "Epoch: 11\n",
      "[Train]\t\t J_a: 268.05, accuracy: 1.00\n",
      "[Validation]\t J_a: 255.88, accuracy: 0.95\n",
      "Epoch: 12\n",
      "[Train]\t\t J_a: 266.50, accuracy: 1.00\n",
      "[Validation]\t J_a: 255.31, accuracy: 0.95\n",
      "Epoch: 13\n",
      "[Train]\t\t J_a: 265.05, accuracy: 1.00\n",
      "[Validation]\t J_a: 253.92, accuracy: 0.95\n",
      "Epoch: 14\n",
      "[Train]\t\t J_a: 263.90, accuracy: 1.00\n",
      "[Validation]\t J_a: 253.43, accuracy: 0.95\n",
      "Epoch: 15\n",
      "[Train]\t\t J_a: 263.00, accuracy: 1.00\n",
      "[Validation]\t J_a: 253.06, accuracy: 0.94\n",
      "Epoch: 16\n",
      "[Train]\t\t J_a: 261.90, accuracy: 1.00\n",
      "[Validation]\t J_a: 252.44, accuracy: 0.95\n",
      "Epoch: 17\n",
      "[Train]\t\t J_a: 260.73, accuracy: 1.00\n",
      "[Validation]\t J_a: 252.17, accuracy: 0.95\n",
      "Epoch: 18\n",
      "[Train]\t\t J_a: 259.97, accuracy: 1.00\n",
      "[Validation]\t J_a: 251.48, accuracy: 0.95\n",
      "Epoch: 19\n",
      "[Train]\t\t J_a: 259.77, accuracy: 1.00\n",
      "[Validation]\t J_a: 250.97, accuracy: 0.95\n",
      "Epoch: 20\n",
      "[Train]\t\t J_a: 258.59, accuracy: 1.00\n",
      "[Validation]\t J_a: 250.93, accuracy: 0.96\n",
      "Epoch: 21\n",
      "[Train]\t\t J_a: 258.05, accuracy: 1.00\n",
      "[Validation]\t J_a: 251.38, accuracy: 0.96\n",
      "Epoch: 22\n",
      "[Train]\t\t J_a: 257.47, accuracy: 1.00\n",
      "[Validation]\t J_a: 250.54, accuracy: 0.96\n",
      "Epoch: 23\n",
      "[Train]\t\t J_a: 257.22, accuracy: 1.00\n",
      "[Validation]\t J_a: 250.25, accuracy: 0.95\n",
      "Epoch: 24\n",
      "[Train]\t\t J_a: 256.22, accuracy: 1.00\n",
      "[Validation]\t J_a: 249.77, accuracy: 0.96\n",
      "Epoch: 25\n",
      "[Train]\t\t J_a: 255.83, accuracy: 1.00\n",
      "[Validation]\t J_a: 250.52, accuracy: 0.96\n",
      "Epoch: 26\n",
      "[Train]\t\t J_a: 255.98, accuracy: 1.00\n",
      "[Validation]\t J_a: 249.81, accuracy: 0.96\n",
      "Epoch: 27\n",
      "[Train]\t\t J_a: 254.76, accuracy: 1.00\n",
      "[Validation]\t J_a: 249.85, accuracy: 0.96\n",
      "Epoch: 28\n",
      "[Train]\t\t J_a: 254.79, accuracy: 1.00\n",
      "[Validation]\t J_a: 249.64, accuracy: 0.96\n",
      "Epoch: 29\n",
      "[Train]\t\t J_a: 253.59, accuracy: 1.00\n",
      "[Validation]\t J_a: 249.44, accuracy: 0.96\n",
      "Epoch: 30\n",
      "[Train]\t\t J_a: 253.80, accuracy: 1.00\n",
      "[Validation]\t J_a: 249.41, accuracy: 0.96\n",
      "Epoch: 31\n",
      "[Train]\t\t J_a: 253.18, accuracy: 1.00\n",
      "[Validation]\t J_a: 249.66, accuracy: 0.96\n",
      "Epoch: 32\n",
      "[Train]\t\t J_a: 253.01, accuracy: 1.00\n",
      "[Validation]\t J_a: 249.13, accuracy: 0.96\n",
      "Epoch: 33\n",
      "[Train]\t\t J_a: 252.58, accuracy: 1.00\n",
      "[Validation]\t J_a: 249.16, accuracy: 0.96\n",
      "Epoch: 34\n",
      "[Train]\t\t J_a: 252.67, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.78, accuracy: 0.96\n",
      "Epoch: 35\n",
      "[Train]\t\t J_a: 252.46, accuracy: 1.00\n",
      "[Validation]\t J_a: 249.02, accuracy: 0.96\n",
      "Epoch: 36\n",
      "[Train]\t\t J_a: 252.03, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.57, accuracy: 0.96\n",
      "Epoch: 37\n",
      "[Train]\t\t J_a: 251.42, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.35, accuracy: 0.96\n",
      "Epoch: 38\n",
      "[Train]\t\t J_a: 251.61, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.42, accuracy: 0.96\n",
      "Epoch: 39\n",
      "[Train]\t\t J_a: 250.98, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.82, accuracy: 0.96\n",
      "Epoch: 40\n",
      "[Train]\t\t J_a: 250.27, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.57, accuracy: 0.96\n",
      "Epoch: 41\n",
      "[Train]\t\t J_a: 250.38, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.40, accuracy: 0.96\n",
      "Epoch: 42\n",
      "[Train]\t\t J_a: 250.01, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.82, accuracy: 0.96\n",
      "Epoch: 43\n",
      "[Train]\t\t J_a: 249.79, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.90, accuracy: 0.96\n",
      "Epoch: 44\n",
      "[Train]\t\t J_a: 249.58, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.73, accuracy: 0.96\n",
      "Epoch: 45\n",
      "[Train]\t\t J_a: 249.65, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.25, accuracy: 0.96\n",
      "Epoch: 46\n",
      "[Train]\t\t J_a: 249.55, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.92, accuracy: 0.97\n",
      "Epoch: 47\n",
      "[Train]\t\t J_a: 248.97, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.70, accuracy: 0.96\n",
      "Epoch: 48\n",
      "[Train]\t\t J_a: 249.23, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.97, accuracy: 0.96\n",
      "Epoch: 49\n",
      "[Train]\t\t J_a: 249.20, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.75, accuracy: 0.96\n",
      "Epoch: 50\n",
      "[Train]\t\t J_a: 249.00, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.15, accuracy: 0.96\n",
      "Epoch: 51\n",
      "[Train]\t\t J_a: 248.36, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.98, accuracy: 0.97\n",
      "Epoch: 52\n",
      "[Train]\t\t J_a: 248.71, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.10, accuracy: 0.96\n",
      "Epoch: 53\n",
      "[Train]\t\t J_a: 248.29, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.21, accuracy: 0.97\n",
      "Epoch: 54\n",
      "[Train]\t\t J_a: 248.87, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.44, accuracy: 0.97\n",
      "Epoch: 55\n",
      "[Train]\t\t J_a: 247.98, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.11, accuracy: 0.97\n",
      "Epoch: 56\n",
      "[Train]\t\t J_a: 247.76, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.95, accuracy: 0.96\n",
      "Epoch: 57\n",
      "[Train]\t\t J_a: 247.94, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.69, accuracy: 0.96\n",
      "Epoch: 58\n",
      "[Train]\t\t J_a: 247.22, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.34, accuracy: 0.97\n",
      "Epoch: 59\n",
      "[Train]\t\t J_a: 247.81, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.02, accuracy: 0.96\n",
      "Epoch: 60\n",
      "[Train]\t\t J_a: 247.61, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.02, accuracy: 0.97\n",
      "Epoch: 61\n",
      "[Train]\t\t J_a: 247.21, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.19, accuracy: 0.97\n",
      "Epoch: 62\n",
      "[Train]\t\t J_a: 247.22, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.74, accuracy: 0.96\n",
      "Epoch: 63\n",
      "[Train]\t\t J_a: 247.24, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.73, accuracy: 0.96\n",
      "Epoch: 64\n",
      "[Train]\t\t J_a: 247.15, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.12, accuracy: 0.96\n",
      "Epoch: 65\n",
      "[Train]\t\t J_a: 247.18, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.56, accuracy: 0.97\n",
      "Epoch: 66\n",
      "[Train]\t\t J_a: 247.04, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.41, accuracy: 0.96\n",
      "Epoch: 67\n",
      "[Train]\t\t J_a: 246.60, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.83, accuracy: 0.96\n",
      "Epoch: 68\n",
      "[Train]\t\t J_a: 246.57, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.63, accuracy: 0.97\n",
      "Epoch: 69\n",
      "[Train]\t\t J_a: 246.80, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.05, accuracy: 0.96\n",
      "Epoch: 70\n",
      "[Train]\t\t J_a: 247.02, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.23, accuracy: 0.96\n",
      "Epoch: 71\n",
      "[Train]\t\t J_a: 246.69, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.99, accuracy: 0.97\n",
      "Epoch: 72\n",
      "[Train]\t\t J_a: 245.79, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.09, accuracy: 0.97\n",
      "Epoch: 73\n",
      "[Train]\t\t J_a: 246.04, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.73, accuracy: 0.97\n",
      "Epoch: 74\n",
      "[Train]\t\t J_a: 245.73, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.55, accuracy: 0.97\n",
      "Epoch: 75\n",
      "[Train]\t\t J_a: 245.54, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.58, accuracy: 0.97\n",
      "Epoch: 76\n",
      "[Train]\t\t J_a: 245.56, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.82, accuracy: 0.97\n",
      "Epoch: 77\n",
      "[Train]\t\t J_a: 245.61, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.36, accuracy: 0.97\n",
      "Epoch: 78\n",
      "[Train]\t\t J_a: 245.61, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.66, accuracy: 0.97\n",
      "Epoch: 79\n",
      "[Train]\t\t J_a: 245.31, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.51, accuracy: 0.96\n",
      "Epoch: 80\n",
      "[Train]\t\t J_a: 245.30, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.52, accuracy: 0.97\n",
      "Epoch: 81\n",
      "[Train]\t\t J_a: 245.32, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.56, accuracy: 0.97\n",
      "Epoch: 82\n",
      "[Train]\t\t J_a: 245.53, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.50, accuracy: 0.97\n",
      "Epoch: 83\n",
      "[Train]\t\t J_a: 245.05, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.41, accuracy: 0.97\n",
      "Epoch: 84\n",
      "[Train]\t\t J_a: 244.87, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.00, accuracy: 0.97\n",
      "Epoch: 85\n",
      "[Train]\t\t J_a: 245.17, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.44, accuracy: 0.97\n",
      "Epoch: 86\n",
      "[Train]\t\t J_a: 244.56, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.28, accuracy: 0.97\n",
      "Epoch: 87\n",
      "[Train]\t\t J_a: 245.15, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.57, accuracy: 0.97\n",
      "Epoch: 88\n",
      "[Train]\t\t J_a: 244.88, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.97, accuracy: 0.97\n",
      "Epoch: 89\n",
      "[Train]\t\t J_a: 244.92, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.38, accuracy: 0.97\n",
      "Epoch: 90\n",
      "[Train]\t\t J_a: 244.88, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.97, accuracy: 0.96\n",
      "Epoch: 91\n",
      "[Train]\t\t J_a: 245.41, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.49, accuracy: 0.97\n",
      "Epoch: 92\n",
      "[Train]\t\t J_a: 244.74, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.53, accuracy: 0.97\n",
      "Epoch: 93\n",
      "[Train]\t\t J_a: 244.41, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.70, accuracy: 0.97\n",
      "Epoch: 94\n",
      "[Train]\t\t J_a: 244.66, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.52, accuracy: 0.97\n",
      "Epoch: 95\n",
      "[Train]\t\t J_a: 244.43, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.95, accuracy: 0.97\n",
      "Epoch: 96\n",
      "[Train]\t\t J_a: 243.92, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.68, accuracy: 0.96\n",
      "Epoch: 97\n",
      "[Train]\t\t J_a: 244.38, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.81, accuracy: 0.97\n",
      "Epoch: 98\n",
      "[Train]\t\t J_a: 244.47, accuracy: 1.00\n",
      "[Validation]\t J_a: 248.00, accuracy: 0.97\n",
      "Epoch: 99\n",
      "[Train]\t\t J_a: 244.31, accuracy: 1.00\n",
      "[Validation]\t J_a: 247.85, accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss, accuracy = (0, 0)\n",
    "    for (x, y), (u, _) in zip(cycle(labelled), unlabelled):\n",
    "        # Wrap in variables\n",
    "        x, y, u = Variable(x), Variable(y), Variable(u)\n",
    "\n",
    "        if cuda:\n",
    "            # They need to be on the same device and be synchronized.\n",
    "            x, y = x.cuda(device=0), y.cuda(device=0)\n",
    "            u = u.cuda(device=0)\n",
    "\n",
    "        L = -elbo(x, y)\n",
    "        U = -elbo(u)\n",
    "\n",
    "        # Add auxiliary classification loss q(y|x)\n",
    "        logits = model.classify(x)\n",
    "        \n",
    "        # Regular cross entropy\n",
    "        classication_loss = torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
    "\n",
    "        J_alpha = L - alpha * classication_loss + U\n",
    "\n",
    "        J_alpha.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += J_alpha.data\n",
    "        accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
    "        \n",
    "    if epoch % 1 == 0:\n",
    "        model.eval()\n",
    "        m = len(unlabelled)\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        print(\"[Train]\\t\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))\n",
    "\n",
    "        total_loss, accuracy = (0, 0)\n",
    "        for x, y in test:\n",
    "            x, y = Variable(x), Variable(y)\n",
    "\n",
    "            if cuda:\n",
    "                x, y = x.cuda(device=0), y.cuda(device=0)\n",
    "\n",
    "            L = -elbo(x, y)\n",
    "            U = -elbo(x)\n",
    "\n",
    "            logits = model.classify(x)\n",
    "            classication_loss = -torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
    "\n",
    "            J_alpha = L + alpha * classication_loss + U\n",
    "\n",
    "            total_loss += J_alpha.data\n",
    "\n",
    "            _, pred_idx = torch.max(logits, 1)\n",
    "            _, lab_idx = torch.max(y, 1)\n",
    "            accuracy += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
    "\n",
    "        m = len(test)\n",
    "        print(\"[Validation]\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e921aa318e5430d00fe89aa1ddefe9ab510ef49c3817fcc19c2997607dbb8d0"
  },
  "kernelspec": {
   "display_name": "dissertation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
